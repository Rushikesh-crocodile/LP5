import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D
from tensorflow.keras.preprocessing.sequence import pad_sequences


# Load the IMDb dataset
max_words = 10000  # Consider only the top 10,000 most frequent words
maxlen = 200  # Maximum sequence length
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_words)


# Pad sequences to ensure uniform length
max_length = 200
X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)
     

# Define the LSTM model
embedding_size = 128
model = Sequential([
    Embedding(input_dim=max_words, output_dim=embedding_size, input_length=max_length),
    SpatialDropout1D(0.2),
    LSTM(100),
    Dense(1, activation='sigmoid')
])



# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


# Train the model
batch_size = 64
epochs = 5
model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=2)


# Evaluate the model on test data
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)



# Make predictions on the test data
predictions = model.predict(X_test)



# Convert predictions to class labels
predicted_labels = np.round(predictions).flatten().astype(int)



# Print the first 10 predictions along with the corresponding true labels
print("Predicted Labels:", predicted_labels[:10])
print("True Labels:", y_test[:10])
